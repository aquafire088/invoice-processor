{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aquafire088/invoice-processor/blob/main/Welcome_To_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio spaces transformers accelerate numpy requests torch torchvision qwen-vl-utils av ipython reportlab fpdf python-docx pillow huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TseNypVUjqQZ",
        "outputId": "fff233c1-21c7-417b-a9bd-9ff6947b8ae7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
            "Collecting spaces\n",
            "  Downloading spaces-0.37.1-py3-none-any.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Collecting qwen-vl-utils\n",
            "  Downloading qwen_vl_utils-0.0.11-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting av\n",
            "  Downloading av-15.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (7.34.0)\n",
            "Collecting reportlab\n",
            "  Downloading reportlab-4.4.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting fpdf\n",
            "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.14)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.1)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: psutil<6,>=2 in /usr/local/lib/python3.11/dist-packages (from spaces) (5.9.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.6.15)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython) (4.9.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython) (0.8.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython) (0.2.13)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading spaces-0.37.1-py3-none-any.whl (31 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qwen_vl_utils-0.0.11-py3-none-any.whl (7.6 kB)\n",
            "Downloading av-15.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (39.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading reportlab-4.4.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fpdf\n",
            "  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=a836e588ad50036965181333234f85e4df76a1f72dacbdb90f3fa895340ff28e\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/66/bbda9866da446a72e206d6484cd97381cbc7859a7068541c36\n",
            "Successfully built fpdf\n",
            "Installing collected packages: fpdf, reportlab, python-docx, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jedi, av, qwen-vl-utils, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, spaces\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed av-15.0.0 fpdf-1.7.2 jedi-0.19.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 python-docx-1.2.0 qwen-vl-utils-0.0.11 reportlab-4.4.2 spaces-0.37.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gradio as gr\n",
        "import os\n",
        "from threading import Thread\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, TextIteratorStreamer\n",
        "from huggingface_hub import login\n",
        "import traceback\n",
        "from getpass import getpass\n",
        "\n",
        "# Import sécurisé pour le modèle Qwen2-VL\n",
        "try:\n",
        "    from transformers.models.qwen2_vl import Qwen2VLForConditionalGeneration\n",
        "    print(\"✅ Qwen2VLForConditionalGeneration importé avec succès\")\n",
        "except ImportError:\n",
        "    try:\n",
        "        from transformers import AutoModelForVision2Seq as Qwen2VLForConditionalGeneration\n",
        "        print(\"⚠️  Utilisation d'AutoModelForVision2Seq comme fallback\")\n",
        "    except ImportError:\n",
        "        print(\"❌ Impossible d'importer le modèle Qwen2-VL\")\n",
        "        raise ImportError(\"Modèle Qwen2-VL non supporté dans cette version de transformers\")\n",
        "\n",
        "# Configuration du modèle\n",
        "MODEL_ID = \"prithivMLmods/Qwen2-VL-OCR-2B-Instruct\"\n",
        "\n",
        "# Authentification Hugging Face sécurisée\n",
        "def setup_huggingface_auth():\n",
        "    \"\"\"Configure l'authentification Hugging Face de manière sécurisée\"\"\"\n",
        "    try:\n",
        "        # Priorité aux variables d'environnement\n",
        "        hf_token = os.getenv('HF_TOKEN') or os.getenv('HUGGINGFACE_TOKEN')\n",
        "\n",
        "        if not hf_token:\n",
        "            print(\"⚠️  Token HF non trouvé dans les variables d'environnement\")\n",
        "            # En mode interactif, demander le token\n",
        "            if hasattr(__builtins__, '__IPYTHON__'):\n",
        "                hf_token = getpass(\"Entrez votre token Hugging Face: \")\n",
        "            else:\n",
        "                print(\"Mode non-interactif: définissez HF_TOKEN dans vos variables d'environnement\")\n",
        "                return False\n",
        "\n",
        "        if hf_token:\n",
        "            login(hf_token)\n",
        "            print(\"✅ Authentification Hugging Face réussie\")\n",
        "            return True\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️  Échec de l'authentification HF: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# Configuration du device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"🖥️  Device utilisé: {device}\")\n",
        "\n",
        "# Initialisation des variables globales\n",
        "MODEL_LOADED = False\n",
        "model = None\n",
        "processor = None\n",
        "\n",
        "def load_model():\n",
        "    \"\"\"Charge le modèle et le processeur\"\"\"\n",
        "    global model, processor, MODEL_LOADED\n",
        "\n",
        "    print(\"🔄 Chargement du modèle en cours...\")\n",
        "    try:\n",
        "        # Authentification\n",
        "        auth_success = setup_huggingface_auth()\n",
        "        if not auth_success:\n",
        "            print(\"⚠️  Tentative de chargement sans authentification\")\n",
        "\n",
        "        # Chargement du modèle\n",
        "        model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "            MODEL_ID,\n",
        "            trust_remote_code=True,\n",
        "            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "            device_map=\"auto\" if device == \"cuda\" else None,\n",
        "            low_cpu_mem_usage=True\n",
        "        ).eval()\n",
        "\n",
        "        # Placement manuel sur CPU si nécessaire\n",
        "        if device == \"cpu\":\n",
        "            model = model.to(device)\n",
        "\n",
        "        # Chargement du processeur\n",
        "        processor = AutoProcessor.from_pretrained(\n",
        "            MODEL_ID,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        print(\"✅ Modèle chargé avec succès!\")\n",
        "        MODEL_LOADED = True\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erreur lors du chargement du modèle: {str(e)}\")\n",
        "        traceback.print_exc()\n",
        "        MODEL_LOADED = False\n",
        "        model = None\n",
        "        processor = None\n",
        "        return False\n",
        "\n",
        "def diagnose_model():\n",
        "    \"\"\"Diagnostic du modèle et de l'environnement\"\"\"\n",
        "    print(\"\\n🔍 DIAGNOSTIC DU SYSTÈME\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Vérification PyTorch\n",
        "    print(f\"PyTorch version: {torch.__version__}\")\n",
        "    print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"Mémoire GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "\n",
        "    # Vérification du modèle\n",
        "    print(f\"Modèle chargé: {MODEL_LOADED}\")\n",
        "    if MODEL_LOADED:\n",
        "        print(f\"Device du modèle: {model.device}\")\n",
        "        print(f\"Type du modèle: {type(model)}\")\n",
        "        print(f\"Processeur: {type(processor)}\")\n",
        "\n",
        "    # Test basique\n",
        "    if MODEL_LOADED:\n",
        "        try:\n",
        "            # Test du tokenizer\n",
        "            test_text = \"Test\"\n",
        "            tokens = processor.tokenizer(test_text, return_tensors=\"pt\")\n",
        "            print(f\"✅ Tokenizer fonctionne: {tokens['input_ids'].shape}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erreur tokenizer: {e}\")\n",
        "\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "# Fonction de test simplifiée\n",
        "def test_model_simple():\n",
        "    \"\"\"Test basique du modèle\"\"\"\n",
        "    if not MODEL_LOADED:\n",
        "        return \"❌ Modèle non chargé\"\n",
        "\n",
        "    try:\n",
        "        # Création d'une image test\n",
        "        test_image = Image.new('RGB', (100, 100), color='white')\n",
        "        test_text = \"Décrivez cette image\"\n",
        "\n",
        "        # Test des inputs\n",
        "        inputs = processor(\n",
        "            text=test_text,\n",
        "            images=[test_image],\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Test de génération minimal\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=10,\n",
        "                do_sample=False\n",
        "            )\n",
        "\n",
        "        result = processor.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return f\"✅ Test réussi: {result[:100]}...\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Test échoué: {str(e)}\"\n",
        "\n",
        "# Extensions d'images supportées\n",
        "SUPPORTED_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp')\n",
        "\n",
        "def format_plain_text(output_text):\n",
        "    \"\"\"Nettoie le texte de sortie\"\"\"\n",
        "    if not output_text:\n",
        "        return \"\"\n",
        "    return output_text.replace(\"<|im_end|>\", \"\").strip()\n",
        "\n",
        "def generate_strict_invoice_prompt(selected_fields):\n",
        "    \"\"\"Génère un prompt strict pour l'extraction de factures\"\"\"\n",
        "    if not selected_fields:\n",
        "        return \"⚠️ Veuillez sélectionner au moins un champ à extraire.\"\n",
        "\n",
        "    field_mapping = {\n",
        "        \"Type de document\": \"type_document\",\n",
        "        \"Numéro de facture\": \"numero_facture\",\n",
        "        \"Référence client\": \"reference_client\",\n",
        "        \"Date de facture\": \"date_facture\",\n",
        "        \"Date d'échéance\": \"date_echeance\",\n",
        "        \"Nom du client\": \"nom_client\",\n",
        "        \"Client final\": \"client_final\",\n",
        "        \"Adresse\": \"adresse_client\",\n",
        "        \"ICE (client/fournisseur)\": \"ice_client\",\n",
        "        \"Référence article\": \"reference_article\",\n",
        "        \"Désignation\": \"designation\",\n",
        "        \"Quantité\": \"quantite\",\n",
        "        \"P.U. Brut\": \"pu_brut\",\n",
        "        \"Remise\": \"remise\",\n",
        "        \"P.U. Net\": \"pu_net\",\n",
        "        \"Montant HT\": \"montant_ht\",\n",
        "        \"TVA\": \"montant_tva\",\n",
        "        \"Montant TTC\": \"montant_ttc\",\n",
        "        \"Montant en lettres\": \"montant_lettres\",\n",
        "        \"Mode de paiement\": \"mode_paiement\",\n",
        "        \"RIB\": \"rib\",\n",
        "        \"Adresse société\": \"adresse_societe\",\n",
        "        \"R.C.\": \"rc\",\n",
        "        \"C.N.S.S\": \"cnss\",\n",
        "        \"Patente\": \"patente\",\n",
        "        \"I.F\": \"if\"\n",
        "    }\n",
        "\n",
        "    valid_fields = [f for f in selected_fields if f in field_mapping]\n",
        "    if not valid_fields:\n",
        "        return \"❌ Aucun champ valide sélectionné.\"\n",
        "\n",
        "    json_structure = \"{\\n\" + \",\\n\".join([f'  \"{field_mapping[f]}\": \"\"' for f in valid_fields]) + \"\\n}\"\n",
        "\n",
        "    prompt = (\n",
        "        \"**Extraction de facture**\\n\\n\"\n",
        "        \"Analysez précisément l'image de facture et extrayez les champs demandés.\\n\\n\"\n",
        "        \"**Champs requis**:\\n\" + \"\\n\".join(f\"- {f}\" for f in valid_fields) +\n",
        "        \"\\n\\n**Format de sortie**:\\n```json\\n\" + json_structure + \"\\n```\\n\\n\"\n",
        "        \"**Instructions**:\\n\"\n",
        "        \"- Retournez uniquement le JSON\\n\"\n",
        "        \"- Utilisez les valeurs exactes de la facture\\n\"\n",
        "        \"- Champs manquants = \\\"\\\"\\n\"\n",
        "        \"- Pas de traduction\\n\"\n",
        "        \"- Formatage numérique exact (ne pas modifier virgules/points)\"\n",
        "    )\n",
        "    return prompt\n",
        "\n",
        "def generate_prompt_from_fields(general, client, items, totals, payment, company):\n",
        "    \"\"\"Combine les champs sélectionnés\"\"\"\n",
        "    all_fields = general + client + items + totals + payment + company\n",
        "    return generate_strict_invoice_prompt(all_fields)\n",
        "\n",
        "def qwen_inference(media_input, text_input):\n",
        "    \"\"\"Exécute l'inférence avec Qwen2-VL\"\"\"\n",
        "    if not MODEL_LOADED or model is None or processor is None:\n",
        "        yield \"❌ Erreur: Modèle non chargé correctement. Veuillez redémarrer l'application.\"\n",
        "        return\n",
        "\n",
        "    if not text_input or not text_input.strip():\n",
        "        yield \"❌ Erreur: Aucun prompt fourni\"\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Gestion de l'image\n",
        "        if media_input is None:\n",
        "            yield \"❌ Erreur: Aucune image fournie\"\n",
        "            return\n",
        "\n",
        "        # Traitement de l'image\n",
        "        image = None\n",
        "        if isinstance(media_input, str):\n",
        "            if not media_input.lower().endswith(SUPPORTED_EXTENSIONS):\n",
        "                yield f\"❌ Format non supporté. Formats acceptés: {', '.join(SUPPORTED_EXTENSIONS)}\"\n",
        "                return\n",
        "            try:\n",
        "                image = Image.open(media_input)\n",
        "            except Exception as e:\n",
        "                yield f\"❌ Erreur d'ouverture de l'image: {str(e)}\"\n",
        "                return\n",
        "        else:\n",
        "            # Cas où media_input est déjà un objet PIL\n",
        "            image = media_input\n",
        "\n",
        "        # Conversion RGB si nécessaire\n",
        "        if image.mode != 'RGB':\n",
        "            image = image.convert('RGB')\n",
        "\n",
        "        yield \"🔄 Traitement de l'image en cours...\"\n",
        "\n",
        "        # Préparation des inputs avec gestion d'erreurs\n",
        "        try:\n",
        "            # Méthode alternative pour Qwen2-VL\n",
        "            inputs = processor(\n",
        "                text=text_input,\n",
        "                images=[image],\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True\n",
        "            )\n",
        "        except Exception as e:\n",
        "            # Fallback avec messages individuels\n",
        "            try:\n",
        "                inputs = processor(\n",
        "                    text=[text_input],\n",
        "                    images=[image],\n",
        "                    return_tensors=\"pt\"\n",
        "                )\n",
        "            except Exception as e2:\n",
        "                yield f\"❌ Erreur lors du traitement des inputs: {str(e2)}\"\n",
        "                return\n",
        "\n",
        "        # Déplacement vers le bon device\n",
        "        try:\n",
        "            inputs = {k: v.to(model.device) if hasattr(v, 'to') else v for k, v in inputs.items()}\n",
        "        except Exception as e:\n",
        "            yield f\"❌ Erreur lors du déplacement vers le device: {str(e)}\"\n",
        "            return\n",
        "\n",
        "        # Méthode sans streaming pour plus de stabilité\n",
        "        try:\n",
        "            yield \"🔄 Génération en cours...\"\n",
        "\n",
        "            # Configuration simplifiée\n",
        "            generation_kwargs = {\n",
        "                **inputs,\n",
        "                \"max_new_tokens\": 512,\n",
        "                \"temperature\": 0.1,\n",
        "                \"do_sample\": False,  # Mode déterministe\n",
        "                \"pad_token_id\": processor.tokenizer.eos_token_id if hasattr(processor.tokenizer, 'eos_token_id') else None\n",
        "            }\n",
        "\n",
        "            # Suppression des paramètres None\n",
        "            generation_kwargs = {k: v for k, v in generation_kwargs.items() if v is not None}\n",
        "\n",
        "            # Génération directe\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(**generation_kwargs)\n",
        "\n",
        "            # Décodage\n",
        "            generated_text = processor.tokenizer.decode(\n",
        "                outputs[0][inputs[\"input_ids\"].shape[1]:],\n",
        "                skip_special_tokens=True\n",
        "            )\n",
        "\n",
        "            # Résultat final\n",
        "            final_result = format_plain_text(generated_text)\n",
        "            if final_result:\n",
        "                yield final_result\n",
        "            else:\n",
        "                yield \"⚠️ Aucun résultat généré\"\n",
        "\n",
        "        except torch.cuda.OutOfMemoryError:\n",
        "            yield \"❌ Erreur: Mémoire GPU insuffisante. Essayez avec une image plus petite.\"\n",
        "            return\n",
        "        except Exception as e:\n",
        "            yield f\"❌ Erreur lors de la génération: {str(e)}\"\n",
        "            print(f\"[DEBUG] Erreur génération: {e}\")\n",
        "            traceback.print_exc()\n",
        "            return\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[DEBUG] Exception générale dans qwen_inference: {e}\")\n",
        "        traceback.print_exc()\n",
        "        yield f\"❌ Erreur lors de l'extraction: {str(e)}\"\n",
        "\n",
        "# Interface Gradio\n",
        "def create_interface():\n",
        "    \"\"\"Crée l'interface Gradio\"\"\"\n",
        "    with gr.Blocks(\n",
        "        title=\"Extracteur de Factures Qwen2\",\n",
        "        css=\"\"\"\n",
        "        .gradio-container {\n",
        "            max-width: 1200px !important;\n",
        "        }\n",
        "        .gr-button-primary {\n",
        "            background: linear-gradient(45deg, #2196F3, #21CBF3) !important;\n",
        "        }\n",
        "        \"\"\"\n",
        "    ) as demo:\n",
        "        gr.Markdown(\"## 📄 Extraction de Données de Factures - Qwen2-VL\")\n",
        "\n",
        "        # Statut du modèle\n",
        "        status_msg = \"✅ Modèle chargé avec succès\" if MODEL_LOADED else \"❌ Modèle non chargé\"\n",
        "        gr.Markdown(f\"**Statut**: {status_msg}\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### 1. Sélection des champs\")\n",
        "                with gr.Group():\n",
        "                    general = gr.CheckboxGroup(\n",
        "                        [\"Type de document\", \"Numéro de facture\", \"Référence client\",\n",
        "                         \"Date de facture\", \"Date d'échéance\"],\n",
        "                        label=\"📋 Informations Générales\"\n",
        "                    )\n",
        "                    client = gr.CheckboxGroup(\n",
        "                        [\"Nom du client\", \"Client final\", \"Adresse\", \"ICE (client/fournisseur)\"],\n",
        "                        label=\"👤 Client\"\n",
        "                    )\n",
        "                    items = gr.CheckboxGroup(\n",
        "                        [\"Référence article\", \"Désignation\", \"Quantité\", \"P.U. Brut\",\n",
        "                         \"Remise\", \"P.U. Net\", \"Montant HT\"],\n",
        "                        label=\"🛍️ Ligne d'articles\"\n",
        "                    )\n",
        "                    totals = gr.CheckboxGroup(\n",
        "                        [\"Montant HT\", \"TVA\", \"Montant TTC\", \"Montant en lettres\"],\n",
        "                        label=\"💰 Totaux\"\n",
        "                    )\n",
        "                    payment = gr.CheckboxGroup(\n",
        "                        [\"Mode de paiement\", \"RIB\"],\n",
        "                        label=\"💳 Paiement\"\n",
        "                    )\n",
        "                    company = gr.CheckboxGroup(\n",
        "                        [\"Adresse société\", \"R.C.\", \"C.N.S.S\", \"Patente\", \"I.F\"],\n",
        "                        label=\"🏢 Coordonnées société\"\n",
        "                    )\n",
        "\n",
        "                generate_btn = gr.Button(\"🔧 Générer le prompt\", variant=\"primary\")\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### 2. Configuration de l'extraction\")\n",
        "                prompt_box = gr.Textbox(\n",
        "                    label=\"📝 Prompt généré\",\n",
        "                    lines=8,\n",
        "                    placeholder=\"Le prompt apparaîtra ici...\",\n",
        "                    show_copy_button=True\n",
        "                )\n",
        "                image_input = gr.Image(\n",
        "                    label=\"📤 Téléverser une facture\",\n",
        "                    type=\"filepath\",\n",
        "                    sources=[\"upload\"]\n",
        "                )\n",
        "                extract_btn = gr.Button(\n",
        "                    \"🚀 Lancer l'extraction\",\n",
        "                    variant=\"primary\",\n",
        "                    size=\"lg\"\n",
        "                )\n",
        "                output_textbox = gr.Textbox(\n",
        "                    label=\"📊 Résultats d'extraction\",\n",
        "                    lines=10,\n",
        "                    interactive=False,\n",
        "                    show_copy_button=True\n",
        "                )\n",
        "\n",
        "        # Gestion des événements\n",
        "        generate_btn.click(\n",
        "            generate_prompt_from_fields,\n",
        "            inputs=[general, client, items, totals, payment, company],\n",
        "            outputs=prompt_box\n",
        "        )\n",
        "\n",
        "        extract_btn.click(\n",
        "            qwen_inference,\n",
        "            inputs=[image_input, prompt_box],\n",
        "            outputs=output_textbox\n",
        "        )\n",
        "\n",
        "    return demo\n",
        "\n",
        "# Lancement de l'application\n",
        "if __name__ == \"__main__\":\n",
        "    demo = create_interface()\n",
        "    demo.launch(\n",
        "\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "mnzxh8ISjr0O",
        "outputId": "851798a0-c4cb-4382-c073-19887754aa45"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Qwen2VLForConditionalGeneration importé avec succès\n",
            "🖥️  Device utilisé: cuda\n",
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://af343d3b5d3526bb74.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://af343d3b5d3526bb74.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Lancement de l'application\n",
        "if __name__ == \"__main__\":\n",
        "    demo = create_interface()\n",
        "    demo.launch(\n",
        "\n",
        "\n",
        "    )"
      ],
      "metadata": {
        "id": "nH1rMPxE2vAk",
        "outputId": "7baf6631-c4ff-4bd9-f7be-504994b102a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://85f0fac366d35d221c.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://85f0fac366d35d221c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def identify_and_save_blob(blob_path):\n",
        "    \"\"\"Identifies if the blob is an image and saves it.\"\"\"\n",
        "    try:\n",
        "        with open(blob_path, 'rb') as file:\n",
        "            blob_content = file.read()\n",
        "            try:\n",
        "                Image.open(io.BytesIO(blob_content)).verify()  # Check if it's a valid image\n",
        "                extension = \".png\"  # Default to PNG for saving\n",
        "                media_type = \"image\"\n",
        "            except (IOError, SyntaxError):\n",
        "                raise ValueError(\"Unsupported media type. Please upload a valid image.\")\n",
        "\n",
        "            filename = f\"temp_{uuid.uuid4()}_media{extension}\"\n",
        "            with open(filename, \"wb\") as f:\n",
        "                f.write(blob_content)\n",
        "\n",
        "            return filename, media_type\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        raise ValueError(f\"The file {blob_path} was not found.\")\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"An error occurred while processing the file: {e}\")\n",
        "\n",
        "@spaces.GPU\n",
        "def qwen_inference(model_name, media_input, text_input=None):\n",
        "    \"\"\"Handles inference for the selected model.\"\"\"\n",
        "    model = models[model_name]\n",
        "    processor = processors[model_name]\n",
        "\n",
        "    if isinstance(media_input, str):\n",
        "        media_path = media_input\n",
        "        if media_path.endswith(tuple([i for i in image_extensions.keys()])):\n",
        "            media_type = \"image\"\n",
        "        else:\n",
        "            try:\n",
        "                media_path, media_type = identify_and_save_blob(media_input)\n",
        "            except Exception as e:\n",
        "                raise ValueError(\"Unsupported media type. Please upload a valid image.\")\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": media_type,\n",
        "                    media_type: media_path\n",
        "                },\n",
        "                {\"type\": \"text\", \"text\": text_input},\n",
        "            ],\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    text = processor.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "    image_inputs, _ = process_vision_info(messages)\n",
        "    inputs = processor(\n",
        "        text=[text],\n",
        "        images=image_inputs,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    streamer = TextIteratorStreamer(\n",
        "        processor.tokenizer, skip_prompt=True, skip_special_tokens=True\n",
        "    )\n",
        "    generation_kwargs = dict(inputs, streamer=streamer, max_new_tokens=1024)\n",
        "\n",
        "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
        "    thread.start()\n",
        "\n",
        "    buffer = \"\"\n",
        "    for new_text in streamer:\n",
        "        buffer += new_text\n",
        "        # Remove <|im_end|> or similar tokens from the output\n",
        "        buffer = buffer.replace(\"<|im_end|>\", \"\")\n",
        "        yield buffer\n",
        "\n",
        "def format_plain_text(output_text):\n",
        "    \"\"\"Formats the output text as plain text without LaTeX delimiters.\"\"\"\n",
        "    # Remove LaTeX delimiters and convert to plain text\n",
        "    plain_text = output_text.replace(\"\\\\(\", \"\").replace(\"\\\\)\", \"\").replace(\"\\\\[\", \"\").replace(\"\\\\]\", \"\")\n",
        "    return plain_text\n",
        "\n",
        "def generate_document(media_path, output_text, file_format, font_size, line_spacing, alignment, image_size):\n",
        "    \"\"\"Generates a document with the input image and plain text output.\"\"\"\n",
        "    plain_text = format_plain_text(output_text)\n",
        "    if file_format == \"pdf\":\n",
        "        return generate_pdf(media_path, plain_text, font_size, line_spacing, alignment, image_size)\n",
        "    elif file_format == \"docx\":\n",
        "        return generate_docx(media_path, plain_text, font_size, line_spacing, alignment, image_size)\n",
        "\n",
        "def generate_pdf(media_path, plain_text, font_size, line_spacing, alignment, image_size):\n",
        "    \"\"\"Generates a PDF document.\"\"\"\n",
        "    filename = f\"output_{uuid.uuid4()}.pdf\"\n",
        "    doc = SimpleDocTemplate(\n",
        "        filename,\n",
        "        pagesize=A4,\n",
        "        rightMargin=inch,\n",
        "        leftMargin=inch,\n",
        "        topMargin=inch,\n",
        "        bottomMargin=inch\n",
        "    )\n",
        "    styles = getSampleStyleSheet()\n",
        "    styles[\"Normal\"].fontSize = int(font_size)\n",
        "    styles[\"Normal\"].leading = int(font_size) * line_spacing\n",
        "    styles[\"Normal\"].alignment = {\n",
        "        \"Left\": 0,\n",
        "        \"Center\": 1,\n",
        "        \"Right\": 2,\n",
        "        \"Justified\": 4\n",
        "    }[alignment]\n",
        "\n",
        "    story = []\n",
        "\n",
        "    # Add image with size adjustment\n",
        "    image_sizes = {\n",
        "        \"Small\": (200, 200),\n",
        "        \"Medium\": (400, 400),\n",
        "        \"Large\": (600, 600)\n",
        "    }\n",
        "    img = RLImage(media_path, width=image_sizes[image_size][0], height=image_sizes[image_size][1])\n",
        "    story.append(img)\n",
        "    story.append(Spacer(1, 12))\n",
        "\n",
        "    # Add plain text output\n",
        "    text = Paragraph(plain_text, styles[\"Normal\"])\n",
        "    story.append(text)\n",
        "\n",
        "    doc.build(story)\n",
        "    return filename\n",
        "\n",
        "def generate_docx(media_path, plain_text, font_size, line_spacing, alignment, image_size):\n",
        "    \"\"\"Generates a DOCX document.\"\"\"\n",
        "    filename = f\"output_{uuid.uuid4()}.docx\"\n",
        "    doc = docx.Document()\n",
        "\n",
        "    # Add image with size adjustment\n",
        "    image_sizes = {\n",
        "        \"Small\": docx.shared.Inches(2),\n",
        "        \"Medium\": docx.shared.Inches(4),\n",
        "        \"Large\": docx.shared.Inches(6)\n",
        "    }\n",
        "    doc.add_picture(media_path, width=image_sizes[image_size])\n",
        "    doc.add_paragraph()\n",
        "\n",
        "    # Add plain text output\n",
        "    paragraph = doc.add_paragraph()\n",
        "    paragraph.paragraph_format.line_spacing = line_spacing\n",
        "    paragraph.paragraph_format.alignment = {\n",
        "        \"Left\": WD_ALIGN_PARAGRAPH.LEFT,\n",
        "        \"Center\": WD_ALIGN_PARAGRAPH.CENTER,\n",
        "        \"Right\": WD_ALIGN_PARAGRAPH.RIGHT,\n",
        "        \"Justified\": WD_ALIGN_PARAGRAPH.JUSTIFY\n",
        "    }[alignment]\n",
        "    run = paragraph.add_run(plain_text)\n",
        "    run.font.size = docx.shared.Pt(int(font_size))\n",
        "\n",
        "    doc.save(filename)\n",
        "    return filename\n",
        "\n",
        "# CSS for output styling\n",
        "css = \"\"\"\n",
        "  #output {\n",
        "    height: 500px;\n",
        "    overflow: auto;\n",
        "    border: 1px solid #ccc;\n",
        "  }\n",
        ".submit-btn {\n",
        "    background-color: #cf3434  !important;\n",
        "    color: white !important;\n",
        "}\n",
        ".submit-btn:hover {\n",
        "    background-color: #ff2323 !important;\n",
        "}\n",
        ".download-btn {\n",
        "    background-color: #35a6d6 !important;\n",
        "    color: white !important;\n",
        "}\n",
        ".download-btn:hover {\n",
        "    background-color: #22bcff !important;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Gradio app setup\n",
        "with gr.Blocks(css=css) as demo:\n",
        "    gr.Markdown(\"# Qwen2VL Models: Vision and Language Processing\")\n",
        "\n",
        "    with gr.Tab(label=\"Image Input\"):\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                model_choice = gr.Dropdown(\n",
        "                    label=\"Model Selection\",\n",
        "                    choices=list(MODEL_OPTIONS.keys()),\n",
        "                    value=\"OCR-KIE\"\n",
        "                )\n",
        "                input_media = gr.File(\n",
        "                    label=\"Upload Image\", type=\"filepath\"\n",
        "                )\n",
        "                text_input = gr.Textbox(label=\"Question\", placeholder=\"Ask a question about the image...\")\n",
        "                submit_btn = gr.Button(value=\"Submit\", elem_classes=\"submit-btn\")\n",
        "\n",
        "            with gr.Column():\n",
        "                output_text = gr.Textbox(label=\"Output Text\", lines=10)\n",
        "                plain_text_output = gr.Textbox(label=\"Standardized Plain Text\", lines=10)\n",
        "\n",
        "        submit_btn.click(\n",
        "            qwen_inference, [model_choice, input_media, text_input], [output_text]\n",
        "        ).then(\n",
        "            lambda output_text: format_plain_text(output_text), [output_text], [plain_text_output]\n",
        "        )\n",
        "\n",
        "        # Add examples directly usable by clicking\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                line_spacing = gr.Dropdown(\n",
        "                    choices=[0.5, 1.0, 1.15, 1.5, 2.0, 2.5, 3.0],\n",
        "                    value=1.5,\n",
        "                    label=\"Line Spacing\"\n",
        "                )\n",
        "                font_size = gr.Dropdown(\n",
        "                    choices=[\"8\", \"10\", \"12\", \"14\", \"16\", \"18\", \"20\", \"22\", \"24\"],\n",
        "                    value=\"18\",\n",
        "                    label=\"Font Size\"\n",
        "                )\n",
        "                alignment = gr.Dropdown(\n",
        "                    choices=[\"Left\", \"Center\", \"Right\", \"Justified\"],\n",
        "                    value=\"Justified\",\n",
        "                    label=\"Text Alignment\"\n",
        "                )\n",
        "                image_size = gr.Dropdown(\n",
        "                    choices=[\"Small\", \"Medium\", \"Large\"],\n",
        "                    value=\"Small\",\n",
        "                    label=\"Image Size\"\n",
        "                )\n",
        "                file_format = gr.Radio([\"pdf\", \"docx\"], label=\"File Format\", value=\"pdf\")\n",
        "                get_document_btn = gr.Button(value=\"Get Document\", elem_classes=\"download-btn\")\n",
        "\n",
        "        get_document_btn.click(\n",
        "            generate_document, [input_media, output_text, file_format, font_size, line_spacing, alignment, image_size], gr.File(label=\"Download Document\")\n",
        "        )\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 988,
          "referenced_widgets": [
            "cabe0f1cafae4b6d8798ee50d902b35c",
            "f6dca76f6e884f8fbc2c61848e501653",
            "3bd1c2a2be2e4a66b50306640931477e",
            "52f45d4494534c2e9550bc00b8fe9ce4",
            "e6b8a4474f1042a297f92e32a0d00318",
            "050cf3897ae34086aeab765e059f31bc",
            "b3bfec37114944859e7c07ec61619a02",
            "46569e33eeb947398697972ff425acd5",
            "85d4d41cd6204c30b886a91a8f417b80",
            "a63d240a03514911a458623b0868f81e",
            "986212a92eea411087d8f413b8e464d9",
            "6003e3fb630845329d01d5d0d2a8a59f",
            "4238239cbb734ab59e1862bf7949f3e6",
            "10a0fef846244e5fa3947fdee48d07e6",
            "9cf2b6715d394b56a4d5a1b3c4c780d7",
            "01ff8e4e19e5438aaec1dc7d5ac012db",
            "065926a7a641456bb9e46c88245425ad",
            "c5c235aa36d04e1a947cb51f47d5c32a",
            "6c9ceaf2ce574eb7a79f465d1c0c24dc",
            "7a513272537745eeb8642998c7050636",
            "682b82a8093a44609712949e29914593",
            "98ec78cc443e48368cd85953b5ab3168",
            "d66841ea1267474f90b53050d0db0661",
            "dc1c1fbbeacd43709ab0269b4356be34",
            "1c4bea1c69db47dc8c6c7e98cb041364",
            "352ff8634e984249887f489c1dc52bde",
            "d160a49f9ecc436780b8b9368ff4f5e7",
            "667fa2fa2f2e41148dca2ab0093355d5",
            "25ba25a1f1244121a6dd12d09b43894f",
            "94a455722bb14b95a2751676af5bcdba",
            "c5a7d975c3f14755af56a532022d64bb",
            "4549a5267a4f4f64a4a2fde6e09d062b",
            "44454465bd5c419fa66e88fb51068e34",
            "583a2ceffa8b4d7fb0a24b3dd09de4bc",
            "474cec8024f34e599d071ce38dc5a040",
            "e151a9d3008d46ca82f43c05223a5102",
            "05e20403eefc401a8e8472edf392647c",
            "588220cb880f4067b9f45444e01ed0b2",
            "0511d3d772554e13b56e51315707c8e8",
            "492826bb5db343e79fd2814ac8c0d992",
            "3ee078a3915841bab1590c603f32a1c3",
            "f02410e4dfbc4960a59fc84bcff5280b",
            "e8da7161087e4a53bc27d0b3fa191733",
            "9a32d2268d7e4478bd881d6ee7488152",
            "4f68ff1348a84d6f8b23f314491b96bd",
            "4801dec086264edca85189c8f797f4fe",
            "76f2dc0754d645c7ba70dd6d3e03ff80",
            "3f8353b6af76413798dd8c2b74d852dd",
            "3ca26cb6039c4b75839b2ece51c27465",
            "ae4391a40893424f88b1853f2fc9b165",
            "03f39c9edc344a7685330ce4f9bab784",
            "7e83772c9a9044978089d96c170d87c0",
            "ebb26d5776374216b248b7aaeab8300e",
            "a24c4e6fff034b9d82b9d5919e768b27",
            "00fdff3acbde42b4afe10b4c07a694f0",
            "8aa421e2ef02472881c0a811ea66a8e6",
            "502e5e71b84b4802808b4bbc5a33923b",
            "07b234ecb78d4706ad1df3b2196083f6",
            "84f7168e094d498d8a02cf8390ccb4fb",
            "7ef8a0ff2f534b2aa4c425cec4083e77",
            "a1717dd2967547029d6b80e298b378ab",
            "9b18329832d34ee08407ce3995bed173",
            "c2334e2c3e01480ca6a1331903767a5d",
            "64a6a9bffd9441d8b29343bfc9e7b41d",
            "e49e1de8b9aa4eb5967f606bbcba3b0e",
            "71fc764cd98b483295b8b241b55b655d",
            "df991cafd385436f919b61b1cf3150ca",
            "0af1ab1c4287465e9d08f84cf248a4ed",
            "9946d11f39874404b0b0829b483930c9",
            "25b9fefa8577458ea0236c6d0262a0ec",
            "3684977bd93c4e7887e317c457a793b8",
            "e508259fc7f246ecb1921f3df09ba07b",
            "64f9d08dfd0e4558b257e6582b52c238",
            "5b891397d2734eeda25e5935629fc935",
            "0fdd7bdc16c243bf99de0630a947f518",
            "cb0b70d426104f95aaeb837aa5597d54",
            "3063fefdb2084603a34f0d62b32f70bb",
            "44c06402309f43d3b6e918bfc3475e99",
            "115fd0e795f4464e8f4d3b1fedc65f19",
            "ce1170090560418499d1f04f82eed190",
            "47152b75b7454efbb2d110fc0e7555e5",
            "3ca6fef715b644a0b1f6913809ea7347",
            "cfcd7ff1348e4722a1b5725829671d0a",
            "3d9183b831674e5681b003f196647688",
            "567cb26172df47bd8f9ee0a3ed573252",
            "3caba063ff264965b2a1fd818eea8b94",
            "3ceb44df5d0d4e058ff0233111f53265",
            "7efa624464cc466290b415ada258cc07"
          ]
        },
        "id": "ovBSsRFhGbs2",
        "outputId": "961676af-f533-42b2-ff39-27a7f0f5d88b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading OCR-KIE...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cabe0f1cafae4b6d8798ee50d902b35c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/4.42G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6003e3fb630845329d01d5d0d2a8a59f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/252 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d66841ea1267474f90b53050d0db0661"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/572 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "583a2ceffa8b4d7fb0a24b3dd09de4bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f68ff1348a84d6f8b23f314491b96bd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8aa421e2ef02472881c0a811ea66a8e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/408 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df991cafd385436f919b61b1cf3150ca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "chat_template.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44c06402309f43d3b6e918bfc3475e99"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://30f5dff2a297203ad5.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://30f5dff2a297203ad5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://30f5dff2a297203ad5.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}